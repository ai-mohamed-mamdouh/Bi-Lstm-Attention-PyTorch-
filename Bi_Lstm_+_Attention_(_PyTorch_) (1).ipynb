{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6D2eKbqvIul"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModel\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import Dataset , DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "token = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "emb_model = AutoModel.from_pretrained( model_name )\n",
        "embedding_matrix = emb_model.embeddings.word_embeddings.weight"
      ],
      "metadata": {
        "id": "a18cf7bc5G0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data**"
      ],
      "metadata": {
        "id": "TSP0PooUXnVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/DataColab/NLP_project/Smart Tech CV Parser & Matcher/DataSets/Preprocessed_Data.txt'\n",
        "data = pd.read_csv( data_path )\n",
        "data.head()"
      ],
      "metadata": {
        "id": "sdRks8BKv-Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Category'].value_counts().shape"
      ],
      "metadata": {
        "id": "NlQ8e9fqkUHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train , test = train_test_split(data , test_size = 0.25 , shuffle = True, stratify=data['Category'])\n",
        "\n",
        "train = train.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "train['Category'] = encoder.fit_transform(train['Category'] )\n",
        "test['Category'] = encoder.transform(test['Category'])\n",
        "\n",
        "train.to_csv('train.csv' , index = False)\n",
        "test.to_csv('test.csv' , index = False)"
      ],
      "metadata": {
        "id": "DNgVl7HL4y3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MUUMLT5gdKeH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loader**"
      ],
      "metadata": {
        "id": "WCj69okB6Hjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Dataset`"
      ],
      "metadata": {
        "id": "AWWt59jo6YPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MyData(Dataset):\n",
        "  def __init__(self, df_path, token, max_len=512, features_col='Text', label_col='Category'):\n",
        "    self.df = pd.read_csv(df_path)\n",
        "    self.x = self.df[features_col].tolist()\n",
        "    self.y = self.df[label_col].tolist()\n",
        "    self.token = token\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = self.x[idx]\n",
        "    label = int(self.y[idx])\n",
        "\n",
        "    enc = self.token(\n",
        "      text,\n",
        "      truncation=True,\n",
        "      padding=False,\n",
        "      max_length=self.max_len,\n",
        "      return_tensors=None\n",
        "    )\n",
        "\n",
        "    input_ids = torch.tensor(enc[\"input_ids\"], dtype=torch.long)  # (T,)\n",
        "    length = input_ids.size(0)\n",
        "\n",
        "    return input_ids, torch.tensor(label, dtype=torch.long), torch.tensor(length, dtype=torch.long)"
      ],
      "metadata": {
        "id": "RrLT6kk5zKKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`collate_fn`"
      ],
      "metadata": {
        "id": "Ua4SRVEw6bFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "PAD_ID = 0\n",
        "\n",
        "def collate_fn(batch):\n",
        "  ids_list, labels, lengths = zip(*batch)\n",
        "\n",
        "  x_padded = pad_sequence(ids_list, batch_first=True, padding_value=PAD_ID)  # (B, T)\n",
        "  y = torch.stack(labels)       # (B,)\n",
        "  lengths = torch.stack(lengths)  # (B,)\n",
        "\n",
        "  return x_padded, y, lengths"
      ],
      "metadata": {
        "id": "_eKWYsIirYOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/train.csv'\n",
        "test_path = '/content/test.csv'\n",
        "train_data = MyData(train_path , token )\n",
        "test_data = MyData(test_path , token )"
      ],
      "metadata": {
        "id": "tDTosr_0Xdgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data now is ids tokens with padding [ 101, 9262, 9722,  ...,    0,    0,    0]\n",
        "train_loader = DataLoader(train_data , batch_size = 16 , collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_data , batch_size = 16 , collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "BCpL7-4GX9XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YhJZi1etd00p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model**"
      ],
      "metadata": {
        "id": "GBrR_BFnXwDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLstmAttention(nn.Module) :\n",
        "\n",
        "  def __init__(self , vocab_size , embedding_dim , embedding_matrix , hidden_dim=128 ,num_class = 43  ) :\n",
        "    super().__init__()\n",
        "\n",
        "    # Embedding\n",
        "    self.embedding = nn.Embedding(vocab_size , embedding_dim , padding_idx=0 )\n",
        "    self.embedding.weight.data.copy_(embedding_matrix)\n",
        "\n",
        "    # Bi Lstm\n",
        "    self.lstm = nn.LSTM(embedding_dim ,\n",
        "                        hidden_dim , batch_first=True ,\n",
        "                        num_layers=2 ,\n",
        "                        bidirectional=True , dropout = 0.3)\n",
        "\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    # Attention\n",
        "    self.attention = nn.Linear(hidden_dim*2 , 1) # for score\n",
        "\n",
        "    # fc\n",
        "    self.fc = nn.Linear(hidden_dim*2 , num_class)\n",
        "\n",
        "\n",
        "  def forward (self ,x , length ) :\n",
        "    emb = self.embedding(x)\n",
        "    emb = self.dropout(emb)\n",
        "\n",
        "    output , _ = self.lstm(emb)\n",
        "\n",
        "    scores = self.attention(output).squeeze(-1)\n",
        "\n",
        "    T = x.size(1)\n",
        "    mask = torch.arange(T, device=x.device).unsqueeze(0) < length.unsqueeze(1)  # (B, T)\n",
        "    scores = scores.masked_fill(~mask, -1e9)\n",
        "\n",
        "    weights = F.softmax(scores ,dim=1)\n",
        "\n",
        "    sent_vec = (output * weights.unsqueeze(-1)).sum(dim=1)  # (B, 2H)\n",
        "\n",
        "    sent_vec = self.dropout(sent_vec)\n",
        "\n",
        "    logits = self.fc(sent_vec)\n",
        "\n",
        "    return logits , weights"
      ],
      "metadata": {
        "id": "bW7j8zwbyzrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = token.vocab_size\n",
        "embedding_dim = 768"
      ],
      "metadata": {
        "id": "s5Ma57rFeug-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BiLstmAttention(vocab_size , embedding_dim , embedding_matrix , hidden_dim=128 ,num_class = 43)"
      ],
      "metadata": {
        "id": "Kf0giNoCvNKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train**"
      ],
      "metadata": {
        "id": "bwaw84q0XyqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train_one_epoch(model, loader):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for x, y , lengths in loader:\n",
        "        x = x.to(device)\n",
        "        lengths = lengths.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # forward\n",
        "        logits, _ = model(x, lengths)\n",
        "\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # accuracy\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    acc = correct / total\n",
        "\n",
        "\n",
        "    print(f\"Train Loss: {avg_loss:.4f} | Train Acc: {acc:.4f}\")"
      ],
      "metadata": {
        "id": "2kug0Qk3vKT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "for i in range(epochs) :\n",
        "  print('epoch : ' ,i+1 )\n",
        "  train_one_epoch(model , train_loader)\n",
        "  print('------------------|------------------')"
      ],
      "metadata": {
        "id": "aOwpVFsawVa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test**"
      ],
      "metadata": {
        "id": "lptepEuCX2iT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def test_model_full(\n",
        "    model,\n",
        "    loader,\n",
        "    num_classes,\n",
        "    device=None,\n",
        "    class_names=None,\n",
        "    plot_roc=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Full evaluation for classification models.\n",
        "\n",
        "    Inputs\n",
        "    ------\n",
        "    model: PyTorch model that returns (logits, attention_weights)\n",
        "    loader: DataLoader yielding (x, lengths, y)\n",
        "    num_classes: int\n",
        "    device: torch.device or None (auto)\n",
        "    class_names: list[str] or None\n",
        "    plot_roc: bool (draw ROC curves)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    results: dict with accuracy, report, confusion_matrix, auc (macro/micro/per_class), roc (fpr/tpr)\n",
        "    \"\"\"\n",
        "\n",
        "    # Lazy imports (so code doesn't crash if you only want basic metrics)\n",
        "    from sklearn.metrics import (\n",
        "        classification_report,\n",
        "        confusion_matrix,\n",
        "        accuracy_score,\n",
        "        roc_auc_score,\n",
        "        roc_curve\n",
        "    )\n",
        "    from sklearn.preprocessing import label_binarize\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "    all_y = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y , lengths in loader:\n",
        "            x = x.to(device)\n",
        "            lengths = lengths.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            logits, _ = model(x, lengths)  # logits: (B, C)\n",
        "            all_logits.append(logits.detach().cpu())\n",
        "            all_y.append(y.detach().cpu())\n",
        "\n",
        "    logits = torch.cat(all_logits, dim=0).numpy()  # (N, C)\n",
        "    y_true = torch.cat(all_y, dim=0).numpy()       # (N,)\n",
        "\n",
        "    # probabilities for ROC/AUC\n",
        "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()  # (N, C)\n",
        "    y_pred = probs.argmax(axis=1)\n",
        "\n",
        "    # ---- Basic metrics ----\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    if class_names is None:\n",
        "        class_names = [str(i) for i in range(num_classes)]\n",
        "\n",
        "    report = classification_report(\n",
        "        y_true, y_pred,\n",
        "        target_names=class_names,\n",
        "        digits=4,\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    results = {\n",
        "        \"accuracy\": acc,\n",
        "        \"classification_report\": report,\n",
        "        \"confusion_matrix\": cm,\n",
        "    }\n",
        "\n",
        "    # ---- ROC / AUC ----\n",
        "    # For multiclass: compute One-vs-Rest ROC and AUC\n",
        "    # y_true_bin: (N, C)\n",
        "    y_true_bin = label_binarize(y_true, classes=list(range(num_classes)))\n",
        "\n",
        "    roc_data = {}\n",
        "    auc_data = {}\n",
        "\n",
        "    # per-class ROC + AUC\n",
        "    per_class_auc = []\n",
        "    for c in range(num_classes):\n",
        "        # If a class doesn't appear in y_true, roc_curve can error.\n",
        "        # We handle it safely.\n",
        "        if y_true_bin[:, c].sum() == 0:\n",
        "            roc_data[c] = {\"fpr\": None, \"tpr\": None}\n",
        "            per_class_auc.append(np.nan)\n",
        "            continue\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_true_bin[:, c], probs[:, c])\n",
        "        roc_data[c] = {\"fpr\": fpr, \"tpr\": tpr}\n",
        "        per_class_auc.append(roc_auc_score(y_true_bin[:, c], probs[:, c]))\n",
        "\n",
        "    # macro/micro AUC (OvR)\n",
        "    # micro: treat every (sample,class) decision as one binary decision\n",
        "    try:\n",
        "        auc_micro = roc_auc_score(y_true_bin, probs, average=\"micro\", multi_class=\"ovr\")\n",
        "    except Exception:\n",
        "        auc_micro = np.nan\n",
        "\n",
        "    # macro: average over classes (ignoring NaNs)\n",
        "    per_class_auc_np = np.array(per_class_auc, dtype=float)\n",
        "    auc_macro = np.nanmean(per_class_auc_np)\n",
        "\n",
        "    auc_data[\"per_class\"] = per_class_auc\n",
        "    auc_data[\"macro_ovr\"] = float(auc_macro) if not np.isnan(auc_macro) else np.nan\n",
        "    auc_data[\"micro_ovr\"] = float(auc_micro) if not np.isnan(auc_micro) else np.nan\n",
        "\n",
        "    results[\"roc\"] = roc_data\n",
        "    results[\"auc\"] = auc_data\n",
        "\n",
        "    # ---- Print summary ----\n",
        "    print(f\"Accuracy: {acc:.4f}\\n\")\n",
        "    print(\"Classification Report:\\n\")\n",
        "    print(report)\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    print(\"\\nAUC (OvR):\")\n",
        "    print(\"  macro:\", auc_data[\"macro_ovr\"])\n",
        "    print(\"  micro:\", auc_data[\"micro_ovr\"])\n",
        "    print(\"  per_class:\", auc_data[\"per_class\"])\n",
        "\n",
        "    # ---- Plot ROC curves (optional) ----\n",
        "    if plot_roc:\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # Plot only classes that have valid ROC\n",
        "        plt.figure()\n",
        "        any_plotted = False\n",
        "        for c in range(num_classes):\n",
        "            fpr = roc_data[c][\"fpr\"]\n",
        "            tpr = roc_data[c][\"tpr\"]\n",
        "            if fpr is None or tpr is None:\n",
        "                continue\n",
        "            any_plotted = True\n",
        "            label = f\"{class_names[c]} (AUC={per_class_auc[c]:.3f})\"\n",
        "            plt.plot(fpr, tpr, label=label)\n",
        "\n",
        "        if any_plotted:\n",
        "            plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Chance\")\n",
        "            plt.xlabel(\"False Positive Rate\")\n",
        "            plt.ylabel(\"True Positive Rate\")\n",
        "            plt.title(\"ROC Curve (One-vs-Rest)\")\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"\\nROC plot skipped: not enough positive samples per class to draw curves.\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "zNixviG1WRc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class_names = [\"class0\", \"class1\", \"...\"]\n",
        "\n",
        "results = test_model_full(\n",
        "    model=model,\n",
        "    loader=test_loader,\n",
        "    num_classes=43,\n",
        "    # class_names=class_names,\n",
        "    plot_roc=True\n",
        ")"
      ],
      "metadata": {
        "id": "yp3fOjXHWVVA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Error Analysis**"
      ],
      "metadata": {
        "id": "WVWeYwMKX63P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def error_analysis(\n",
        "    model,\n",
        "    loader,\n",
        "    tokenizer,\n",
        "    class_names=None,\n",
        "    device=None,\n",
        "\n",
        "    # what to return\n",
        "    max_errors=100,\n",
        "    topk_tokens=12,\n",
        "    confidence_threshold=0.0,   # keep wrong preds with conf >= threshold\n",
        "\n",
        "    # plots\n",
        "    plot=True,\n",
        "    n_attn_plots=10,\n",
        "    top_confusions=15,          # bar chart: top confusing pairs\n",
        "\n",
        "    # attention display\n",
        "    merge_wordpieces=True,\n",
        "    ignore_special=True,\n",
        "\n",
        "    # export\n",
        "    export_dir=None,            # e.g. \"outputs\" or None\n",
        "    export_prefix=\"errors\"\n",
        "):\n",
        "    \"\"\"\n",
        "    ERROR ANALYSIS ONLY (no ROC/AUC, no full test suite).\n",
        "    - Collects misclassified samples (sorted by highest confidence first)\n",
        "    - Shows top confusing true->pred pairs\n",
        "    - Visualizes:\n",
        "        1) Wrong-confidence histogram\n",
        "        2) Confusion pairs bar chart\n",
        "        3) Attention heatmaps for top wrong samples\n",
        "      ...and prints an English explanation after each figure.\n",
        "    - Optional export to JSON/CSV + summary JSON.\n",
        "\n",
        "    Assumptions:\n",
        "      loader yields (x, lengths, y)\n",
        "      model(x, lengths) returns (logits, attention_weights)\n",
        "      x are BERT input_ids (so we can convert ids->tokens with tokenizer)\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------- helpers ----------\n",
        "    def _safe_label(i: int) -> str:\n",
        "        if class_names is None:\n",
        "            return str(i)\n",
        "        return class_names[i] if 0 <= i < len(class_names) else str(i)\n",
        "\n",
        "    def _merge_wordpieces(tokens, weights):\n",
        "        \"\"\"\n",
        "        Merge WordPiece tokens: [\"play\", \"##ing\"] -> [\"playing\"]\n",
        "        Aggregate attention weights by summing, then renormalize.\n",
        "        \"\"\"\n",
        "        merged_tokens, merged_weights = [], []\n",
        "        cur_tok, cur_w = \"\", 0.0\n",
        "\n",
        "        for tok, w in zip(tokens, weights):\n",
        "            if tok.startswith(\"##\") and cur_tok != \"\":\n",
        "                cur_tok += tok[2:]\n",
        "                cur_w += w\n",
        "            else:\n",
        "                if cur_tok != \"\":\n",
        "                    merged_tokens.append(cur_tok)\n",
        "                    merged_weights.append(cur_w)\n",
        "                cur_tok = tok\n",
        "                cur_w = w\n",
        "\n",
        "        if cur_tok != \"\":\n",
        "            merged_tokens.append(cur_tok)\n",
        "            merged_weights.append(cur_w)\n",
        "\n",
        "        s = sum(merged_weights)\n",
        "        if s > 0:\n",
        "            merged_weights = [mw / s for mw in merged_weights]\n",
        "\n",
        "        return merged_tokens, merged_weights\n",
        "\n",
        "    def _topk(tokens, weights, k):\n",
        "        k = min(k, len(tokens))\n",
        "        if k <= 0:\n",
        "            return []\n",
        "        idx = np.argsort(-np.array(weights))[:k]\n",
        "        return [{\"token\": tokens[i], \"weight\": float(weights[i]), \"index\": int(i)} for i in idx]\n",
        "\n",
        "    # ---------- device ----------\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    special = set(tokenizer.all_special_tokens) if ignore_special else set()\n",
        "\n",
        "    # ---------- collect wrong predictions only ----------\n",
        "    errors = []\n",
        "    confusion_counter = Counter()\n",
        "    wrong_confidences = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        global_index = 0  # index across the whole loader\n",
        "\n",
        "        for x, lengths, y in loader:\n",
        "            x = x.to(device)\n",
        "            lengths = lengths.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            logits, attn = model(x, lengths)     # logits (B,C), attn (B,T)\n",
        "            probs = F.softmax(logits, dim=1)     # (B,C)\n",
        "            preds = probs.argmax(dim=1)          # (B,)\n",
        "            confs = probs.max(dim=1).values      # (B,)\n",
        "\n",
        "            B = x.size(0)\n",
        "            for i in range(B):\n",
        "                true_id = int(y[i].item())\n",
        "                pred_id = int(preds[i].item())\n",
        "                conf = float(confs[i].item())\n",
        "\n",
        "                if pred_id != true_id and conf >= confidence_threshold:\n",
        "                    # tokens + attention for this sample\n",
        "                    T = int(lengths[i].item())\n",
        "                    ids = x[i].detach().cpu().tolist()[:T]\n",
        "                    tokens = tokenizer.convert_ids_to_tokens(ids)\n",
        "\n",
        "                    weights = attn[i].detach().cpu().tolist()[:T]\n",
        "                    s = sum(weights)\n",
        "                    if s > 0:\n",
        "                        weights = [w / s for w in weights]\n",
        "\n",
        "                    # remove special tokens\n",
        "                    if ignore_special:\n",
        "                        kept = [(t, w) for t, w in zip(tokens, weights) if t not in special]\n",
        "                        tokens_kept = [t for t, _ in kept]\n",
        "                        weights_kept = [w for _, w in kept]\n",
        "                    else:\n",
        "                        tokens_kept, weights_kept = tokens, weights\n",
        "\n",
        "                    # merge wordpieces\n",
        "                    if merge_wordpieces:\n",
        "                        tokens_final, weights_final = _merge_wordpieces(tokens_kept, weights_kept)\n",
        "                    else:\n",
        "                        tokens_final, weights_final = tokens_kept, weights_kept\n",
        "\n",
        "                    top_tokens = _topk(tokens_final, weights_final, topk_tokens)\n",
        "\n",
        "                    errors.append({\n",
        "                        \"global_index\": int(global_index + i),\n",
        "                        \"true_id\": true_id,\n",
        "                        \"true_label\": _safe_label(true_id),\n",
        "                        \"pred_id\": pred_id,\n",
        "                        \"pred_label\": _safe_label(pred_id),\n",
        "                        \"confidence\": conf,\n",
        "                        \"length\": int(T),\n",
        "                        \"top_tokens_by_attention\": top_tokens,\n",
        "                        \"tokens\": tokens_final,\n",
        "                        \"attention\": [float(a) for a in weights_final],\n",
        "                    })\n",
        "\n",
        "                    wrong_confidences.append(conf)\n",
        "                    confusion_counter[(true_id, pred_id)] += 1\n",
        "\n",
        "            global_index += B\n",
        "\n",
        "    # sort errors: highest confidence wrong first\n",
        "    errors.sort(key=lambda e: e[\"confidence\"], reverse=True)\n",
        "    errors = errors[:max_errors]\n",
        "\n",
        "    # top confusion pairs\n",
        "    confusions = []\n",
        "    for (t, p), c in confusion_counter.most_common(top_confusions):\n",
        "        confusions.append({\n",
        "            \"true_id\": int(t), \"true_label\": _safe_label(int(t)),\n",
        "            \"pred_id\": int(p), \"pred_label\": _safe_label(int(p)),\n",
        "            \"count\": int(c)\n",
        "        })\n",
        "\n",
        "    summary = {\n",
        "        \"wrong_found_total\": int(sum(confusion_counter.values())),\n",
        "        \"returned_errors\": int(len(errors)),\n",
        "        \"confidence_threshold\": float(confidence_threshold),\n",
        "        \"top_confusions\": confusions,\n",
        "    }\n",
        "\n",
        "    print(f\"Wrong samples found: {summary['wrong_found_total']} | \"\n",
        "          f\"Returned: {summary['returned_errors']} (conf >= {confidence_threshold})\")\n",
        "\n",
        "    # ---------- plots + English explanations ----------\n",
        "    if plot:\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # 1) Wrong-confidence histogram\n",
        "        if len(wrong_confidences) > 0:\n",
        "            plt.figure(figsize=(7, 4))\n",
        "            plt.hist(wrong_confidences, bins=25)\n",
        "            plt.title(\"Wrong Predictions — Confidence Histogram\")\n",
        "            plt.xlabel(\"Confidence (max softmax probability)\")\n",
        "            plt.ylabel(\"Count\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            print(\"\\n[Figure 1 Explanation — Wrong Confidence Histogram]\")\n",
        "            print(\"- This shows how confident the model was on the samples it MISCLASSIFIED.\")\n",
        "            print(\"- If many errors have high confidence (e.g., > 0.7), the model is overconfident and making 'dangerous' mistakes.\")\n",
        "            print(\"- If most errors have low confidence, the model is uncertain (less risky) when it makes mistakes.\")\n",
        "            print(\"How to use it:\")\n",
        "            print(\"- High-confidence errors: inspect data quality/labels, add regularization, or improve features/model.\")\n",
        "            print(\"- You can set a confidence threshold in production to reject uncertain predictions.\\n\")\n",
        "\n",
        "        # 2) Top confusion pairs bar chart\n",
        "        if len(confusions) > 0:\n",
        "            labels = [f\"{c['true_label']} → {c['pred_label']}\" for c in confusions]\n",
        "            counts = [c[\"count\"] for c in confusions]\n",
        "\n",
        "            plt.figure(figsize=(10, max(4, 0.35 * len(labels))))\n",
        "            plt.barh(labels[::-1], counts[::-1])\n",
        "            plt.title(\"Top Confusion Pairs (True → Predicted)\")\n",
        "            plt.xlabel(\"Number of Mistakes\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            print(\"[Figure 2 Explanation — Top Confusion Pairs]\")\n",
        "            print(\"- Each bar shows the most common 'true class → predicted class' mistakes.\")\n",
        "            print(\"- This tells you which classes the model confuses the most.\")\n",
        "            print(\"How to use it:\")\n",
        "            print(\"- If class A is often predicted as class B, compare their training examples: they may overlap in meaning.\")\n",
        "            print(\"- Add more data for those classes, improve labeling rules, or redesign class definitions if needed.\")\n",
        "            print(\"- You can also consider class-specific weighting or focal loss if imbalance is an issue.\\n\")\n",
        "\n",
        "        # 3) Attention heatmaps for some errors\n",
        "        for idx_plot, e in enumerate(errors[:min(n_attn_plots, len(errors))], start=1):\n",
        "            tokens_plot = e[\"tokens\"]\n",
        "            att_plot = np.array(e[\"attention\"], dtype=float)\n",
        "\n",
        "            plt.figure(figsize=(max(8, len(tokens_plot) * 0.35), 2.2))\n",
        "            plt.imshow(att_plot.reshape(1, -1), aspect=\"auto\")\n",
        "            plt.yticks([])\n",
        "            plt.xticks(np.arange(len(tokens_plot)), tokens_plot, rotation=45, ha=\"right\")\n",
        "            plt.colorbar()\n",
        "            plt.title(\n",
        "                f\"Attention Heatmap (WRONG #{idx_plot}) | true={e['true_label']} \"\n",
        "                f\"pred={e['pred_label']} conf={e['confidence']:.2f}\"\n",
        "            )\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            print(f\"[Figure 3.{idx_plot} Explanation — Attention Heatmap for a Wrong Prediction]\")\n",
        "            print(\"- Darker/stronger colors mean the model gave more attention to those tokens.\")\n",
        "            print(\"- This helps you understand *what evidence* the model used when it made a wrong decision.\")\n",
        "            print(\"How to use it:\")\n",
        "            print(\"- If attention focuses on irrelevant words (e.g., stopwords) or on artifacts, improve preprocessing or training data.\")\n",
        "            print(\"- If attention highlights misleading keywords, you may need more diverse examples or better class separation.\")\n",
        "            print(\"- If you see special tokens or padding receiving attention, your masking/lengths logic is likely wrong.\\n\")\n",
        "\n",
        "    # ---------- export ----------\n",
        "    if export_dir is not None:\n",
        "        os.makedirs(export_dir, exist_ok=True)\n",
        "\n",
        "        json_path = os.path.join(export_dir, f\"{export_prefix}.json\")\n",
        "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump({\"summary\": summary, \"errors\": errors}, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        csv_path = os.path.join(export_dir, f\"{export_prefix}.csv\")\n",
        "        with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\n",
        "                \"global_index\",\"true_id\",\"true_label\",\"pred_id\",\"pred_label\",\n",
        "                \"confidence\",\"length\",\"top_tokens_by_attention\"\n",
        "            ])\n",
        "            for e in errors:\n",
        "                top_str = \"; \".join([f\"{t['token']}:{t['weight']:.4f}\" for t in e[\"top_tokens_by_attention\"]])\n",
        "                writer.writerow([\n",
        "                    e[\"global_index\"], e[\"true_id\"], e[\"true_label\"],\n",
        "                    e[\"pred_id\"], e[\"pred_label\"],\n",
        "                    f\"{e['confidence']:.6f}\", e[\"length\"], top_str\n",
        "                ])\n",
        "\n",
        "        summary_path = os.path.join(export_dir, f\"{export_prefix}_summary.json\")\n",
        "        with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"✅ Exported:\\n- {json_path}\\n- {csv_path}\\n- {summary_path}\")\n",
        "\n",
        "    return errors, confusions, summary"
      ],
      "metadata": {
        "id": "giCbK_00X-vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errors, confusions, summary = error_analysis(\n",
        "    model=model,\n",
        "    loader=test_loader,\n",
        "    tokenizer=token,\n",
        "    # class_names=class_names,     # optional\n",
        "    confidence_threshold=0.7,\n",
        "    max_errors=100,\n",
        "    n_attn_plots=10,\n",
        "    plot=True,\n",
        "    export_dir=\"outputs\",\n",
        "    export_prefix=\"bilstm_attn_errors\"\n",
        ")"
      ],
      "metadata": {
        "id": "2WuN6IhYYzku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save Model**"
      ],
      "metadata": {
        "id": "Cl-zMknE8ma-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.state_dict(), \"bilstm_attention.pth\")"
      ],
      "metadata": {
        "id": "xGGPWGHYeiYV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}